{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67e067b",
   "metadata": {},
   "source": [
    "# Part 3: ECG Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this part, you'll work with the MIT-BIH Arrhythmia Database to build a model for heartbeat classification using a simple neural network architecture. This will help you understand how to apply neural networks to time series data in healthcare.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Load and preprocess ECG time series data\n",
    "- Implement a simple neural network for sequence classification\n",
    "- Train and evaluate the model\n",
    "- Interpret results in a clinical context\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb6a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt\n",
    "%pip install wfdb  # For reading MIT-BIH format\n",
    "\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import wfdb\n",
    "from scipy import signal\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results/part_3', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "def download_mitbih_dataset():\n",
    "    \"\"\"\n",
    "    Download and extract MIT-BIH Arrhythmia Database.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the extracted dataset\n",
    "    \"\"\"\n",
    "    data_dir = 'data/mitdb'\n",
    "    if os.path.exists(data_dir):\n",
    "        print(\"Dataset already downloaded.\")\n",
    "        return data_dir\n",
    "    \n",
    "    print(\"Downloading MIT-BIH Arrhythmia Database...\")\n",
    "    url = \"https://www.physionet.org/static/published-projects/mitdb/mit-bih-arrhythmia-database-1.0.0.zip\"\n",
    "    zip_path = 'data/mitdb.zip'\n",
    "    \n",
    "    # Download dataset\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    \n",
    "    # Extract dataset\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "    \n",
    "    # Clean up\n",
    "    os.remove(zip_path)\n",
    "    print(\"Dataset downloaded and extracted successfully.\")\n",
    "    return data_dir\n",
    "\n",
    "# Download dataset\n",
    "data_dir = download_mitbih_dataset()\n",
    "\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import wfdb\n",
    "from scipy import signal\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results/part_3', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb78d3c",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "### Task 1.1: Load ECG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ecg_data(record_path):\n",
    "    \"\"\"\n",
    "    Load ECG data from MIT-BIH database.\n",
    "    \n",
    "    Args:\n",
    "        record_path: Path to the record file\n",
    "    \n",
    "    Returns:\n",
    "        signals: ECG signals\n",
    "        annotations: Beat annotations\n",
    "    \"\"\"\n",
    "    # Read record\n",
    "    record = wfdb.rdrecord(record_path)\n",
    "    signals = record.p_signal\n",
    "    \n",
    "    # Read annotations\n",
    "    ann = wfdb.rdann(record_path, 'atr')\n",
    "    annotations = ann.symbol\n",
    "    \n",
    "    return signals, annotations\n",
    "\n",
    "def verify_data_loading(signals, annotations):\n",
    "    \"\"\"\n",
    "    Verify that the data is loaded correctly.\n",
    "    \n",
    "    Args:\n",
    "        signals: ECG signals\n",
    "        annotations: Beat annotations\n",
    "    \"\"\"\n",
    "    # Plot sample ECG segment\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(signals[:1000, 0])\n",
    "    plt.title('Sample ECG Segment')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print data statistics\n",
    "    print(f\"Signal shape: {signals.shape}\")\n",
    "    print(f\"Number of annotations: {len(annotations)}\")\n",
    "    print(f\"Unique beat types: {np.unique(annotations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fdac9e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Task 1.2: Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ecg(signals, annotations, window_size=180):\n",
    "    \"\"\"\n",
    "    Preprocess ECG data for model input.\n",
    "    \n",
    "    Args:\n",
    "        signals: ECG signals\n",
    "        annotations: Beat annotations\n",
    "        window_size: Size of the window around each beat\n",
    "    \n",
    "    Returns:\n",
    "        X: Preprocessed signals\n",
    "        y: Labels\n",
    "    \"\"\"\n",
    "    # Normalize signals\n",
    "    signals = (signals - np.mean(signals)) / np.std(signals)\n",
    "    \n",
    "    # Extract beats\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i, ann in enumerate(annotations):\n",
    "        if ann in ['N', 'L', 'R', 'A', 'V']:  # Normal and abnormal beats\n",
    "            # Get window around beat\n",
    "            start = max(0, i - window_size//2)\n",
    "            end = min(len(signals), i + window_size//2)\n",
    "            \n",
    "            # Pad if necessary\n",
    "            if start == 0:\n",
    "                pad_left = window_size//2 - i\n",
    "                segment = np.pad(signals[start:end], ((pad_left, 0), (0, 0)))\n",
    "            elif end == len(signals):\n",
    "                pad_right = window_size//2 - (len(signals) - i)\n",
    "                segment = np.pad(signals[start:end], ((0, pad_right), (0, 0)))\n",
    "            else:\n",
    "                segment = signals[start:end]\n",
    "            \n",
    "            X.append(segment)\n",
    "            \n",
    "            # Convert annotation to label\n",
    "            if ann == 'N':\n",
    "                y.append(0)  # Normal\n",
    "            else:\n",
    "                y.append(1)  # Abnormal\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def verify_preprocessing(X, y):\n",
    "    \"\"\"\n",
    "    Verify that the preprocessing is correct.\n",
    "    \n",
    "    Args:\n",
    "        X: Preprocessed signals\n",
    "        y: Labels\n",
    "    \"\"\"\n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "    print(f\"Label distribution: {np.bincount(y)}\")\n",
    "    \n",
    "    # Plot sample beats\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.plot(X[i, :, 0])\n",
    "        plt.title(f'Beat Type: {\"Normal\" if y[i] == 0 else \"Abnormal\"}')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca9301",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 2. Model Implementation\n",
    "\n",
    "### Task 2.1: Create Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4759a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_nn(input_shape):\n",
    "    \"\"\"\n",
    "    Create a simple neural network for ECG classification.\n",
    "    This is similar to the architecture from Part 1.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input data\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        # Flatten input\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        \n",
    "        # Dense layers\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def verify_model_architecture(model):\n",
    "    \"\"\"\n",
    "    Verify that the model architecture meets requirements.\n",
    "    \n",
    "    Args:\n",
    "        model: Keras model\n",
    "    \"\"\"\n",
    "    model.summary()\n",
    "    \n",
    "    # Test model with sample input\n",
    "    sample_input = tf.random.normal((1, 180, 2))  # window_size x channels\n",
    "    sample_output = model(sample_input)\n",
    "    print(f\"\\nSample output shape: {sample_output.shape}\")\n",
    "    \n",
    "    # Verify architecture requirements\n",
    "    assert any('dense' in layer.name for layer in model.layers), \"Model must include dense layers\"\n",
    "    assert any('dropout' in layer.name for layer in model.layers), \"Model must include dropout\"\n",
    "    assert model.loss == 'binary_crossentropy', \"Model must use binary crossentropy loss\"\n",
    "    assert any('auc' in metric.name for metric in model.metrics), \"Model must include AUC metric\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd42cf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 3. Training and Evaluation\n",
    "\n",
    "### Task 3.1: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    \"\"\"\n",
    "    Train the model and save it.\n",
    "    \n",
    "    Args:\n",
    "        model: Keras model\n",
    "        X_train: Training data\n",
    "        y_train: Training labels\n",
    "        X_val: Validation data\n",
    "        y_val: Validation labels\n",
    "        model_name: Name for saving the model\n",
    "    \n",
    "    Returns:\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            f'models/{model_name}.keras',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def verify_training(history):\n",
    "    \"\"\"\n",
    "    Verify that the training was successful.\n",
    "    \n",
    "    Args:\n",
    "        history: Training history\n",
    "    \"\"\"\n",
    "    # Plot training curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'], label='Training')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c979b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Task 3.2: Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model and save metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        X_test: Test data\n",
    "        y_test: Test labels\n",
    "        model_name: Name for saving metrics\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Evaluate model\n",
    "    test_loss, test_accuracy, test_auc = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    predicted_labels = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = tf.math.confusion_matrix(y_test, predicted_labels)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    tn, fp, fn, tp = cm.numpy().ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'accuracy': float(test_accuracy),\n",
    "        'auc': float(test_auc),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'confusion_matrix': cm.numpy().tolist()\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    os.makedirs('results/part_3', exist_ok=True)\n",
    "    with open(f'results/part_3/{model_name}_metrics.txt', 'w') as f:\n",
    "        f.write(f\"model: {model_name}\\n\")\n",
    "        f.write(f\"accuracy: {metrics['accuracy']}\\n\")\n",
    "        f.write(f\"auc: {metrics['auc']}\\n\")\n",
    "        f.write(f\"precision: {metrics['precision']}\\n\")\n",
    "        f.write(f\"recall: {metrics['recall']}\\n\")\n",
    "        f.write(f\"f1_score: {metrics['f1_score']}\\n\")\n",
    "        f.write(f\"confusion_matrix: {metrics['confusion_matrix']}\\n\")\n",
    "        f.write(\"----\\n\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def verify_evaluation(metrics):\n",
    "    \"\"\"\n",
    "    Verify that the evaluation is complete.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary of metrics\n",
    "    \"\"\"\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Test AUC: {metrics['auc']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3daf75",
   "metadata": {},
   "source": [
    "## 4. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load and preprocess data\n",
    "    record_path = 'mitdb/100'  # Example record\n",
    "    signals, annotations = load_ecg_data(record_path)\n",
    "    verify_data_loading(signals, annotations)\n",
    "    \n",
    "    # Preprocess data\n",
    "    X, y = preprocess_ecg(signals, annotations)\n",
    "    verify_preprocessing(X, y)\n",
    "    \n",
    "    # Split data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 2. Create and verify model\n",
    "    model = create_simple_nn(input_shape=(180, 2))\n",
    "    verify_model_architecture(model)\n",
    "    \n",
    "    # 3. Train and evaluate\n",
    "    model_name = 'ecg_classifier'\n",
    "    history = train_model(model, X_train, y_train, X_val, y_val, model_name)\n",
    "    verify_training(history)\n",
    "    \n",
    "    # Save model\n",
    "    model.save(f'models/{model_name}.keras')\n",
    "    \n",
    "    # Evaluate and save metrics\n",
    "    metrics = evaluate_model(model, X_test, y_test, model_name)\n",
    "    verify_evaluation(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c7178c",
   "metadata": {},
   "source": [
    "## Progress Checkpoints\n",
    "\n",
    "1. **Data Loading**:\n",
    "   - [ ] Successfully download MIT-BIH dataset\n",
    "   - [ ] Load and visualize ECG signals\n",
    "   - [ ] Verify signal shape and annotations\n",
    "\n",
    "2. **Preprocessing**:\n",
    "   - [ ] Normalize signals\n",
    "   - [ ] Extract beat windows\n",
    "   - [ ] Verify window shapes and labels\n",
    "\n",
    "3. **Model Implementation**:\n",
    "   - [ ] Create simple neural network\n",
    "   - [ ] Verify model architecture\n",
    "   - [ ] Test model output shape\n",
    "\n",
    "4. **Training**:\n",
    "   - [ ] Train model with callbacks\n",
    "   - [ ] Monitor training progress\n",
    "   - [ ] Save best model\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - [ ] Calculate performance metrics\n",
    "   - [ ] Save metrics in correct format\n",
    "   - [ ] Visualize results\n",
    "\n",
    "## Intended Endpoint\n",
    "\n",
    "1. **Model Performance**:\n",
    "   - Accuracy > 75% on test set\n",
    "   - AUC > 0.80\n",
    "   - F1-score > 0.70\n",
    "\n",
    "2. **Required Files**:\n",
    "   - `models/ecg_classifier.keras`\n",
    "   - `results/part_3/ecg_classifier_metrics.txt`\n",
    "\n",
    "3. **Metrics Format**:\n",
    "   ```\n",
    "   model: ecg_classifier\n",
    "   accuracy: float\n",
    "   auc: float\n",
    "   precision: float\n",
    "   recall: float\n",
    "   f1_score: float\n",
    "   confusion_matrix: [[TN, FP], [FN, TP]]\n",
    "   ----\n",
    "   ```\n",
    "\n",
    "4. **Model Architecture**:\n",
    "   - Must use at least 2 dense layers\n",
    "   - Must include dropout layers\n",
    "   - Must use binary crossentropy loss\n",
    "   - Must include AUC metric"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
